{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8e4c7d898f74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff35c057b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8e4c7d898f74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p_df = pd.read_csv('data/users.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>24</th>\n",
       "      <th>M</th>\n",
       "      <th>technician</th>\n",
       "      <th>85711</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>M</td>\n",
       "      <td>executive</td>\n",
       "      <td>98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  24  M  technician  85711\n",
       "0  2  53  F       other  94043\n",
       "1  3  23  M      writer  32067\n",
       "2  4  24  M  technician  43537\n",
       "3  5  33  F       other  15213\n",
       "4  6  42  M   executive  98101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create spark object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AS RDD object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1|24|M|technician|85711',\n",
       " '2|53|F|other|94043',\n",
       " '3|23|M|writer|32067',\n",
       " '4|24|M|technician|43537',\n",
       " '5|33|F|other|15213']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does .map do?\n",
    "rdd = sc.textFile(\"data/users.csv\").map(lambda line: line.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '24', 'M', 'technician', '85711'],\n",
       " ['2', '53', 'F', 'other', '94043'],\n",
       " ['3', '23', 'M', 'writer', '32067'],\n",
       " ['4', '24', 'M', 'technician', '43537'],\n",
       " ['5', '33', 'F', 'other', '15213']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... \n",
    "### Take 5 minutes to explore various methods of rdd\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a DF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/users.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='1', _c1='24', _c2='M', _c3='technician', _c4='85711'),\n",
       " Row(_c0='2', _c1='53', _c2='F', _c3='other', _c4='94043'),\n",
       " Row(_c0='3', _c1='23', _c2='M', _c3='writer', _c4='32067'),\n",
       " Row(_c0='4', _c1='24', _c2='M', _c3='technician', _c4='43537'),\n",
       " Row(_c0='5', _c1='33', _c2='F', _c3='other', _c4='15213')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's already a DF, but this is the easy way to rename columns\n",
    "df = (spark.read.csv(\"data/users.csv\", sep=\"|\")\n",
    "           .toDF(\"id\", \"age\", \"gender\", \"occupation\", \"zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='1', age='24', gender='M', occupation='technician', zip='85711'),\n",
       " Row(id='2', age='53', gender='F', occupation='other', zip='94043'),\n",
       " Row(id='3', age='23', gender='M', occupation='writer', zip='32067'),\n",
       " Row(id='4', age='24', gender='M', occupation='technician', zip='43537'),\n",
       " Row(id='5', age='33', gender='F', occupation='other', zip='15213')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   occupation|count|\n",
      "+-------------+-----+\n",
      "|      student|  196|\n",
      "|     educator|   95|\n",
      "|administrator|   79|\n",
      "|     engineer|   67|\n",
      "|   programmer|   66|\n",
      "|    librarian|   51|\n",
      "|       writer|   45|\n",
      "|    executive|   32|\n",
      "|    scientist|   31|\n",
      "|       artist|   28|\n",
      "|   technician|   27|\n",
      "|    marketing|   26|\n",
      "|entertainment|   18|\n",
      "|   healthcare|   16|\n",
      "|      retired|   14|\n",
      "|     salesman|   12|\n",
      "|       lawyer|   12|\n",
      "|         none|    9|\n",
      "|    homemaker|    7|\n",
      "|       doctor|    7|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# putting into parathensis allows us not to worry about extra line spaces\n",
    "(\n",
    "    df.where(\"occupation != 'other'\")\n",
    "      .groupby(\"occupation\")\n",
    "      .count()\n",
    "      .sort(\"count\", ascending=0)\n",
    "      .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, age: string, gender: string, occupation: string, zip: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.persist \n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>85711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id age gender  occupation    zip\n",
       "0  1  24      M  technician  85711\n",
       "1  2  53      F       other  94043\n",
       "2  3  23      M      writer  32067\n",
       "3  4  24      M  technician  43537\n",
       "4  5  33      F       other  15213"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()\n",
    "#df.head()\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show(df, n=5):\n",
    "#     return df.limit(n).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT occupation)|\n",
      "+--------------------------+\n",
      "|                        21|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.countDistinct('occupation')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   occupation|count|\n",
      "+-------------+-----+\n",
      "|      student|  196|\n",
      "|        other|  105|\n",
      "|     educator|   95|\n",
      "|administrator|   79|\n",
      "|     engineer|   67|\n",
      "|   programmer|   66|\n",
      "|    librarian|   51|\n",
      "|       writer|   45|\n",
      "|    executive|   32|\n",
      "|    scientist|   31|\n",
      "+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT occupation, COUNT(*) as count\n",
    "FROM users\n",
    "GROUP BY occupation\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "df.createOrReplaceTempView('users')\n",
    "\n",
    "output = spark.sql(query)\n",
    "\n",
    "output.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic = spark.read.csv(path='data/titanic_clean.csv', sep=',',\n",
    "                     encoding='UTF-8', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show(df, n=5):\n",
    "    return df.limit(n).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do some basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'int'),\n",
       " ('Survived', 'int'),\n",
       " ('Pclass', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int'),\n",
       " ('Fare', 'double'),\n",
       " ('Embarked', 'string')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            7         0       1   \n",
       "6            8         0       3   \n",
       "7            9         1       3   \n",
       "8           10         1       2   \n",
       "9           11         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "6                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "7  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "8                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "9                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "\n",
       "   Parch     Fare Embarked  \n",
       "0      0   7.2500        S  \n",
       "1      0  71.2833        C  \n",
       "2      0   7.9250        S  \n",
       "3      0  53.1000        S  \n",
       "4      0   8.0500        S  \n",
       "5      0  51.8625        S  \n",
       "6      1  21.0750        S  \n",
       "7      2  11.1333        S  \n",
       "8      0  30.0708        C  \n",
       "9      1  16.7000        S  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  259|\n",
      "|  male|  453|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic.groupBy('Sex').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new column is female \"sex\"\n",
    "from pyspark.sql.functions import when, col\n",
    "titanic = titanic.withColumn(\"is_female\",\n",
    "                     (when(col(\"Sex\").like(\"%female%\"), 1)\n",
    "                      .otherwise(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# drop original target column\n",
    "titanic = titanic.drop(\"Name\", \"Sex\", \"Embarked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set survived as label and lowercase column names\n",
    "titanic = titanic.selectExpr(\"Survived as label\", \"PassengerId as p_id\" , \"Survived as survived\", \"Pclass as pclass\", \"is_female as is_female\", \"Age as age\", \"SibSp as sib\", \"Parch as parch\", \"Fare as fare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>p_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>is_female</th>\n",
       "      <th>age</th>\n",
       "      <th>sib</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  p_id  survived  pclass  is_female   age  sib  parch     fare\n",
       "0      0     1         0       3          0  22.0    1      0   7.2500\n",
       "1      1     2         1       1          1  38.0    1      0  71.2833\n",
       "2      1     3         1       3          1  26.0    0      0   7.9250\n",
       "3      1     4         1       1          1  35.0    1      0  53.1000\n",
       "4      0     5         0       3          0  35.0    0      0   8.0500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(titanic, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Features and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,3.0,0.0,22.0...|    0|\n",
      "|[2.0,1.0,1.0,38.0...|    1|\n",
      "|[3.0,3.0,1.0,26.0...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = [\"p_id\", \"pclass\", \"is_female\", \"age\", \"sib\", \"parch\", \"fare\"], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(titanic)\n",
    "vhouse_df = vhouse_df.select(['features', 'label'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning in Spark\n",
    "\n",
    "Spark's [documentation](https://spark.apache.org/docs/2.2.0/ml-guide.html#mllib-main-guide) is fairly straight forward!  Let's take a look. It shouldn't look *too* different than `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 487\n",
      "Test Dataset Count: 225\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = vhouse_df.randomSplit([0.7, 0.3], seed=100)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.00012857786439025406,-0.29141359929279465,0.9439338621577505,-0.008204924609738736,-0.053221641160273225,0.04905300527354308,0.0023391835621167554]\n",
      "Intercept: -0.057088887132624014\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol='label', maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lr_model = lr.fit(training_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary has many components one can call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8282070517629403"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "training_summary = lr_model.summary\n",
    "\n",
    "training_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC: 0.8282070517629403\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "print(\"areaUnderROC: \" + str(training_summary.areaUnderROC))\n",
    "\n",
    "#training_summary.roc.show(n=10, truncate=15)\n",
    "roc = training_summary.roc.toPandas()\n",
    "plt.plot(roc['FPR'], roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Area Under ROC: 0.8282070517629403\n",
      "Test: Area Under ROC: 0.8896062490036672\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "print(\"Training: Area Under ROC: \" + str(training_summary.areaUnderROC))\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "#### Specify and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Area Under ROC: 0.6621233859397417\n"
     ]
    }
   ],
   "source": [
    "# select example rows to display.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions,\n",
    "                                                        {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree\n",
    "#### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "\n",
    "# Train model with Training Data\n",
    "dtModel = dt.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  11\n",
      "depth =  3\n"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = dtModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Area Under ROC: 0.7354136776661884\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions,\n",
    "                                                        {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "#### Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\",\n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees=100,\n",
    "                            maxDepth=4,\n",
    "                            maxBins=32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Score and evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score test Data\n",
    "predictions = rfModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Area Under ROC: 0.9034353578829906\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions,\n",
    "                                                        {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing grid search with `CrossValidator` in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Area Under ROC: 0.9060656783038423\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 200])  # number of trees\n",
    "             .addGrid(rf.maxDepth, [3, 4, 5])       # maximum depth\n",
    "             .addGrid(rf.maxBins, [24, 32, 40])   # Number of bins\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                    estimatorParamMaps=param_grid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cv_model = cv.fit(training_data)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions,\n",
    "                                                        {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which model had the best AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals in review. How did we do?\n",
    "- Align the relationships between Hadoop and Spark\n",
    "- Differentiate between Spark RDDs and Spark Dataframes and when each is appropriate\n",
    "- Locate and explore the Spark.ML documentation\n",
    "- Code along to see how pyspark is similar differnt to python (sklearn/pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
